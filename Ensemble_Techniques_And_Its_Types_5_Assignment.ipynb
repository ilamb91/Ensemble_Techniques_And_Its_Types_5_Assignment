{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d86499-abb2-4b8a-a4da-7b480f2e3e10",
   "metadata": {},
   "source": [
    "# Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "• Use an automated feature selection method to identify the important features in the dataset.\n",
    "\n",
    "• Create a numerical pipeline that includes the following steps:\n",
    "\n",
    "• Impute the missing values in the numerical columns using the mean of the column values.\n",
    "\n",
    "• Scale the numerical columns using standardisation.\n",
    "\n",
    "• Create a categorical pipeline that includes the following steps:\n",
    "\n",
    "• Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "\n",
    "• One-hot encode the categorical columns.\n",
    "\n",
    "• Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "\n",
    "• Use a Random Forest Classifier to build the final model.\n",
    "\n",
    "• Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd61f33-5db9-4d5c-b9a1-68c07bbc6f23",
   "metadata": {},
   "source": [
    "A1\n",
    "\n",
    "Creating a machine learning pipeline for feature engineering, preprocessing, and modeling is a common practice in data science. Below, I'll outline the steps you described and provide Python code snippets for each part of the pipeline. I'll assume you're using scikit-learn for this purpose.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Separate the target variable (y) from the features (X)\n",
    "X = data.drop(columns=['target_column'])\n",
    "y = data['target_column']\n",
    "\n",
    "# Step 1: Automated Feature Selection\n",
    "# Use a feature selection method to identify important features\n",
    "# In this example, we use SelectFromModel with a RandomForestClassifier\n",
    "feature_selector = SelectFromModel(RandomForestClassifier(n_estimators=100))\n",
    "X_selected = feature_selector.fit_transform(X, y)\n",
    "\n",
    "# Step 2: Preprocessing Pipelines for Numerical and Categorical Data\n",
    "# Create a numerical pipeline\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create a categorical pipeline\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Step 3: Combine Numerical and Categorical Pipelines\n",
    "# Use ColumnTransformer to apply the pipelines to the respective feature sets\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Step 4: Build the Final Model\n",
    "# Add the preprocessor and the classifier (Random Forest) to a final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
    "])\n",
    "\n",
    "# Step 5: Split Data and Train the Model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy on test data: {accuracy:.2f}')\n",
    "```\n",
    "\n",
    "**Interpretation and Possible Improvements:**\n",
    "\n",
    "- The pipeline handles feature selection, missing value imputation, scaling for numerical features, and one-hot encoding for categorical features.\n",
    "\n",
    "- The Random Forest Classifier is used as the final model.\n",
    "\n",
    "- The pipeline's performance is evaluated using accuracy on the test dataset.\n",
    "\n",
    "**Possible Improvements:**\n",
    "\n",
    "1. **Hyperparameter Tuning:** Optimize hyperparameters for the Random Forest Classifier and other components of the pipeline using techniques like grid search or random search.\n",
    "\n",
    "2. **Feature Selection:** Experiment with different feature selection methods or consider using feature importance scores from the Random Forest for feature selection.\n",
    "\n",
    "3. **Handling Class Imbalance:** If the dataset is imbalanced, consider techniques like resampling (oversampling or undersampling) or using different evaluation metrics (e.g., F1-score) to handle class imbalance.\n",
    "\n",
    "4. **Advanced Imputation:** Explore more advanced imputation techniques, such as K-nearest neighbors imputation, especially for missing categorical data.\n",
    "\n",
    "5. **Ensemble Methods:** Consider using ensemble methods like stacking or boosting to improve model performance further.\n",
    "\n",
    "6. **Feature Engineering:** Depending on your domain knowledge, you may explore additional feature engineering steps to create new meaningful features.\n",
    "\n",
    "Remember that the choice of preprocessing steps and model should be guided by the specific characteristics of your dataset and problem domain. The pipeline can be further refined and customized based on your domain knowledge and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f255a8-e399-4485-bae3-32d1740ed8b8",
   "metadata": {},
   "source": [
    "# Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f729e-d08e-4760-8af4-991878e2f9b1",
   "metadata": {},
   "source": [
    "A2\n",
    "\n",
    "To build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions on the Iris dataset, you can follow these steps using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3a85ed-4d4e-4b13-a464-9a2e1725893b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Voting Classifier: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_classifier),\n",
    "        ('lr', lr_classifier)\n",
    "    ],\n",
    "    voting='hard'  # 'hard' for majority voting, 'soft' for weighted voting based on class probabilities\n",
    ")\n",
    "\n",
    "# Train the Voting Classifier on the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the Voting Classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of Voting Classifier: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de528c3-6623-4205-9bfc-c602ff84c53f",
   "metadata": {},
   "source": [
    "The Voting Classifier combines the predictions from both classifiers and provides an accuracy score. You can adjust the hyperparameters of the individual classifiers and the Voting Classifier as needed for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45764e54-4fdd-4ded-9515-f3a4e5700e45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
